{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp, pearsonr\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_theme('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"starmpcc/Asclepius-7B\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures = [\n",
    "    'hysterectomy',\n",
    "    'cystoscopy',\n",
    "    'c-section|cesarean',\n",
    "    'salpingo-oophorectomy',\n",
    "    'delivery',\n",
    "    'placement',\n",
    "    'drainage',\n",
    "    'resection',\n",
    "    'myomectomy',\n",
    "    'omentectomy',\n",
    "    'salpingectomy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_procedures(results_df: pd.DataFrame):\n",
    "    for procedure in procedures:\n",
    "        results_df[procedure] = results_df['procedure'].str.lower().str.contains(procedure, regex=True)\n",
    "    return results_df\n",
    "\n",
    "def get_quantity_for_experiment(results_df: pd.DataFrame, experiment_name: str):\n",
    "    \"\"\" Get mean yes / (yes+no) guess probs, and total yes + no prob\n",
    "    \"\"\"\n",
    "    yes_prob = results_df[f'{experiment_name}_Yes']\n",
    "    no_prob = results_df[f'{experiment_name}_No']\n",
    "    quantity = yes_prob / (yes_prob + no_prob)\n",
    "    return quantity, yes_prob, no_prob, yes_prob + no_prob\n",
    "\n",
    "def get_results_for_df(results_df: pd.DataFrame, subset_name: str=None):\n",
    "    baseline_quantity, baseline_yes_prob, baseline_no_prob, baseline_yes_plus_no = get_quantity_for_experiment(results_df, 'prompt')\n",
    "    results_df['baseline_quantity'] = baseline_quantity\n",
    "\n",
    "    for experiment in ['F->M', 'F->NB', 'F->TM', 'random_proc']:\n",
    "        experiment_quantity, experiment_yes_prob, experiment_no_prob, experiment_yes_plus_no = get_quantity_for_experiment(results_df, experiment)\n",
    "        results_df[f'{experiment}_quantity'] = experiment_quantity\n",
    "        experiment_diffs = baseline_quantity - experiment_quantity\n",
    "        results_df[f'{experiment}_diffs'] = experiment_diffs\n",
    "        ttest_res = ttest_1samp(experiment_diffs, popmean=0)\n",
    "        p_val = ttest_res.pvalue\n",
    "        teststat = ttest_res.statistic\n",
    "        print(f\" & {baseline_quantity.mean():.3f} ({baseline_quantity.std():.3f}) & {experiment} & {experiment_diffs.mean():.3f} ({experiment_diffs.std():.3f}) & {teststat:.3f} & {p_val:.4f} \\\\\\\\\")\n",
    "        # print(f\"Experiment: {experiment}; mean diff: {experiment_diffs.mean()}; std diff: {experiment_diffs.mean()}; test stat: {teststat}; p-val: {p_val:.5f}\")\n",
    "        if experiment != 'random_proc':\n",
    "            results_df[f'{experiment}_dist_frac'] = results_df[f'{experiment}_dist'] / results_df['tokenized_note_len']\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_results(model_name: str):\n",
    "    print(model_name)\n",
    "    model_name = model_name.replace('/', '__')\n",
    "    results_df = pd.read_csv(f'results/{model_name}_initial_results.csv')\n",
    "    dists_df = pd.read_csv(f'edit_dists/{model_name}_edit_dists.csv')\n",
    "    results_df = pd.merge(results_df, dists_df)\n",
    "    results_df = characterize_procedures(results_df)\n",
    "    augmented_results_df = get_results_for_df(results_df)\n",
    "    augmented_results_df['model'] = model_name.split('__')[-1]\n",
    "    # for procedure in procedures:\n",
    "    #     results_sub_df = results_df[results_df[procedure] == True]\n",
    "    #     print(procedure, f\"N={len(results_sub_df)}\")\n",
    "    #     get_results_for_df(results_sub_df)\n",
    "    # results_sub_df = results_df.copy()\n",
    "    # for procedure in procedures:\n",
    "    #     results_sub_df = results_sub_df[~results_sub_df[procedure]]\n",
    "    # print('other_procedure', f\"N={len(results_sub_df)}\")\n",
    "    # get_results_for_df(results_sub_df)\n",
    "    return augmented_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for model in models:\n",
    "    all_results.append(analyze_results(model))\n",
    "complete_results_df = pd.concat(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results2(model_name: str):\n",
    "    print(model_name)\n",
    "    model_name = model_name.replace('/', '__')\n",
    "    results_df = pd.read_csv(f'results/{model_name}_prompt_engineering_results.csv')\n",
    "    dists_df = pd.read_csv(f'edit_dists/{model_name}_edit_dists.csv')\n",
    "    results_df = pd.merge(results_df, dists_df)\n",
    "\n",
    "    results_df = characterize_procedures(results_df)\n",
    "    augmented_results_df = get_results_for_df(results_df)\n",
    "    augmented_results_df['model'] = model_name.split('__')[-1]\n",
    "    # for procedure in procedures:\n",
    "    #     results_sub_df = results_df[results_df[procedure] == True]\n",
    "    #     print(procedure, f\"N={len(results_sub_df)}\")\n",
    "    #     get_results_for_df(results_sub_df)\n",
    "    # results_sub_df = results_df.copy()\n",
    "    # for procedure in procedures:\n",
    "    #     results_sub_df = results_sub_df[~results_sub_df[procedure]]\n",
    "    # print('other_procedure', f\"N={len(results_sub_df)}\")\n",
    "    # get_results_for_df(results_sub_df)\n",
    "    return augmented_results_df\n",
    "pe_results = []\n",
    "for model in models:\n",
    "    pe_results.append(analyze_results2(model))\n",
    "pe_results_df = pd.concat(pe_results)\n",
    "pe_results_df = pe_results_df[['note_id', 'F->M_diffs','random_proc_diffs', 'model']]\n",
    "pe_results_df = pe_results_df.rename({'F->M_diffs': 'F->M_diffs_PE', 'random_proc_diffs': 'random_proc_diffs_PE'}, axis='columns')\n",
    "complete_results_df = pd.merge(complete_results_df, pe_results_df, on=['note_id', 'model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    model_results_df = complete_results_df[complete_results_df['model'] == model_name.split('/')[-1]]\n",
    "    # print(model_name, pearsonr(model_results_df['F->M_dist'], model_results_df['F->M_diffs']))\n",
    "    # print(model_name, pearsonr(model_results_df['F->M_dist_frac'], model_results_df['F->M_diffs']))\n",
    "    correlation_results = pearsonr(model_results_df['F->M_dist_frac'], model_results_df['F->M_diffs'])\n",
    "    print(f\"{model_name.split('/')[-1]} & {correlation_results.statistic:.3f} & {correlation_results.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cols = [f'{exp}_diffs' for exp in ['F->M', 'F->NB', 'F->TM', 'random_proc']]\n",
    "melted_df = complete_results_df.melt(id_vars=['model', 'note_id'], value_name='diffs', var_name='experiment')\n",
    "melted_df_diffs = melted_df[melted_df['experiment'].isin(experiment_cols)]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "g = sns.barplot(data=melted_df_diffs, hue='experiment', x='model', y='diffs', ax=ax)\n",
    "plt.title(\"Mean Difference in Predicted Probability (Experiment - Baseline)\")\n",
    "g.get_legend().set_title(\"Experiment\")\n",
    "# replace labels\n",
    "new_labels = ['F->M', 'F->NB', 'F->TM', 'Random Procedure']\n",
    "for t, l in zip(g.get_legend().texts, new_labels):\n",
    "    t.set_text(l)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "melted_df_pe = melted_df[melted_df['experiment'].isin(['F->M_diffs', 'F->M_diffs_PE', 'random_proc_diffs', 'random_proc_diffs_PE'])]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "g = sns.barplot(data=melted_df_pe, hue='experiment', x='model', y='diffs', ax=ax)\n",
    "plt.title(\"Effects of Prompt Engineering\")\n",
    "g.get_legend().set_title(\"Experiment\")\n",
    "new_labels = ['F->M', 'Random Procedure', 'F->M (PE)', 'Random Procedure (PE)']\n",
    "for t, l in zip(g.get_legend().texts, new_labels):\n",
    "    t.set_text(l)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.histplot(data=complete_results_df, x='F->M_diffs', hue='model', multiple='dodge', bins=10)\n",
    "plt.title(\"Histogram in changes from Baseline to Male Note\")\n",
    "plt.show()\n",
    "\n",
    "experiment_cols = []\n",
    "for exp in ['prompt']:\n",
    "    experiment_cols.extend([f'{exp}_Yes', f'{exp}_No'])\n",
    "melted_df_pred_probs = melted_df[melted_df['experiment'].isin(experiment_cols)]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "g = sns.barplot(data=melted_df_pred_probs, hue='experiment', x='model', y='diffs', ax=ax)\n",
    "plt.title(\"Mean Predicted Probability for Yes/No\")\n",
    "g.get_legend().set_title(\"Experiment\")\n",
    "# replace labels\n",
    "new_labels = ['Predicted \"Yes\" Prob', 'Predicted \"No\" Prob']\n",
    "for t, l in zip(g.get_legend().texts, new_labels):\n",
    "    t.set_text(l)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(x='model', y='random_proc_diffs', data=complete_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [model_name.split('/')[-1] for model_name in models]\n",
    "model_sizes = [1, 3, 8, 7, 7]\n",
    "ftm_diffs = [0.03135433031490543, 0.06281644184168893, 0.09766994673834288, 0.004116367901499969, 0.1820370526985932]\n",
    "random_proc_diffs = [0.0021958206397623362, 0.05308820133602772, 0.19659108637701223, 0.0973688137570994, 0.20461204953487638]\n",
    "architecture = ['Llama', 'Llama', 'Llama', 'Asclepius', 'Mistral']\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0,0.25)\n",
    "ax.set_ylim(0,0.25)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "df = pd.DataFrame({'model_name': a, 'Mean F->M Decrease': b, 'Mean Randomized Procedure Decrease': c, 'Model Size\\n(Billion Params)': d} for a,b,c,d in zip(model_names, ftm_diffs, random_proc_diffs, model_sizes))\n",
    "sns.scatterplot(data=df, x='Mean Randomized Procedure Decrease', y='Mean F->M Decrease', size='Model Size\\n(Billion Params)', style=architecture, hue=architecture, ax=ax)\n",
    "for i, txt in enumerate(model_names):\n",
    "    ax.annotate(txt, (random_proc_diffs[i], ftm_diffs[i]))\n",
    "plt.legend([],[], frameon=False)\n",
    "ax.set_xlabel(ax.get_xlabel()+\"\\n(Higher is Better)\")\n",
    "ax.set_ylabel(ax.get_ylabel()+\"\\n(Lower is Better)\")\n",
    "plt.title(\"Decrease in Predicted Probability for a Random Procedure vs. for F->M Transform\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,3))\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_results_df = complete_results_df[complete_results_df['model'] == model_name]\n",
    "    x = np.linspace(0,1,1000)\n",
    "    y_orig = [sum(model_results_df['baseline_quantity'] > thresh) / len(model_results_df) for thresh in x]\n",
    "    y_changed = [sum(model_results_df['F->M_quantity'] > thresh) / len(model_results_df) for thresh in x]\n",
    "    ax = axes[i]\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.plot(x, y_orig)\n",
    "    ax.plot(x, y_changed)\n",
    "    \n",
    "    \n",
    "plt.suptitle(\"Recall for Original vs. Recall for F->M\")\n",
    "plt.legend([\"original\", \"swapped\"])\n",
    "fig.supxlabel(\"Threshold\")\n",
    "fig.supylabel(\"Recall\")\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.tight_layout()\n",
    "print(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    dists_df = pd.read_csv(f'edit_dists/{model.replace('/', '__')}_edit_dists.csv')\n",
    "    for col in ['F->M_dist', 'F->NB_dist', 'F->TM_dist']:\n",
    "        print(model, col, (dists_df[col] / dists_df['tokenized_note_len']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXP_biomedclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
